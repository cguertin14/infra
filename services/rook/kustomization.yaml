apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: rook-ceph

resources:
- resources/namespace.yml

patches:
- path: patches/storageclasses.yml
- path: patches/affinity.yml

labels:
- includeSelectors: true
  pairs:
    app.kubernetes.io/name: rook-ceph

helmCharts:
# Operator helm chart
- name: rook-ceph
  namespace: rook-ceph
  releaseName: rook-ceph
  repo: https://charts.rook.io/release
  valuesInline:
    crds:
      enabled: true
    monitoring:
      enabled: true
  version: v1.16.5
# Ceph cluster helm chart
- name: rook-ceph-cluster
  namespace: rook-ceph
  releaseName: rook-ceph-cluster
  repo: https://charts.rook.io/release
  version: v1.16.5
  valuesInline:
    # Size is 3 because of the triple node cluster - which consists of the nvidiaserver, mediaserver & miniitx0 machines.
    configOverride: |
      [global]
      osd_pool_default_size = 3

    dashboard:
      enabled: true
      urlPrefix: /ceph-dashboard
      port: 8443
      ssl: true

    ingress:
      enabled: true
      dashboard:
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt-issuer
          nginx.ingress.kubernetes.io/auth-signin: https://auth.k8s.cguertin.dev/oauth2/start?rd=https://$host$uri
          nginx.ingress.kubernetes.io/auth-url: https://auth.k8s.cguertin.dev/oauth2/auth
        host:
          name: rook-ceph.k8s.cguertin.dev
          path: /ceph-dashboard(/|$)(.*)
          pathType: ImplementationSpecific
        tls:
        - hosts:
          - rook-ceph.k8s.cguertin.dev
          secretName: rook-ceph-dashboard-cert

    mon:
      allowMultiplePerNode: false
      count: 3
    
    #
    # Monitor deployments may contain an anti-affinity rule for avoiding monitor
    # collocation on the same node. This is a required rule when host network is used
    # or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
    # preferred rule with weight: 50.
    #
    # The above placement information can also be specified for mon, mgr, cleanup and osd components
    #
    # mon:
    # mgr:
    # cleanup:
    # osd:
    monitoring:
      createPrometheusRules: true
      enabled: true

    nfs:
      enabled: false

    operatorNamespace: rook-ceph

    toolbox:
      enabled: false

    resources:
      mgr:
        # limits:
        #   memory: "1Gi"
        requests:
          cpu: "500m"
          memory: "512Mi"
      mon:
        # limits:
        #   memory: "2Gi"
        requests:
          cpu: "1000m"
          memory: "1Gi"
      osd:
        # limits:
        #   memory: "4Gi"
        requests:
          cpu: "1000m"
          memory: "4Gi"
      prepareosd:
        # limits: It is not recommended to set limits on the OSD prepare job
        #         since it's a one-time burst for memory that must be allowed to
        #         complete without an OOM kill.  Note however that if a k8s
        #         limitRange guardrail is defined external to Rook, the lack of
        #         a limit here may result in a sync failure, in which case a
        #         limit should be added.  1200Mi may suffice for up to 15Ti
        #         OSDs ; for larger devices 2Gi may be required.
        #         cf. https://github.com/rook/rook/pull/11103
        requests:
          cpu: "500m"
          memory: "50Mi"
      mgr-sidecar:
        # limits:
        #   memory: "100Mi"
        requests:
          cpu: "100m"
          memory: "40Mi"
      crashcollector:
        # limits:
        #   memory: "60Mi"
        requests:
          cpu: "100m"
          memory: "60Mi"
      logcollector:
        # limits:
        #   memory: "1Gi"
        requests:
          cpu: "100m"
          memory: "100Mi"
      cleanup:
        # limits:
        #   memory: "1Gi"
        requests:
          cpu: "500m"
          memory: "100Mi"
      exporter:
        # limits:
        #   memory: "128Mi"
        requests:
          cpu: "50m"
          memory: "50Mi"
