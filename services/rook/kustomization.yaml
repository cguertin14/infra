apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: rook-ceph

resources:
- resources/namespace.yml

patches:
- path: patches/storageclasses.yml
- path: patches/cluster-affinity.yml
- path: patches/cluster-resources.yml

labels:
- includeSelectors: true
  pairs:
    app.kubernetes.io/name: rook-ceph

helmCharts:
# Operator helm chart
- name: rook-ceph
  namespace: rook-ceph
  releaseName: rook-ceph
  repo: https://charts.rook.io/release
  valuesInline:
    crds:
      enabled: true
    monitoring:
      enabled: true
    pluginNodeAffinity: kubernetes.io/arch=amd64 # only run plugin pods on amd64 machines
    csiCephFSPluginResource: |
      - name : driver-registrar
        resource:
          requests:
            memory: 128Mi
            cpu: 50m
          limits:
            memory: 256Mi
      - name : csi-cephfsplugin
        resource:
          requests:
            memory: 512Mi
            cpu: 250m
          limits:
            memory: 512Mi
      - name : liveness-prometheus
        resource:
          requests:
            memory: 128Mi
            cpu: 50m
          limits:
            memory: 256Mi
    csiRBDPluginResource: |
      - name : driver-registrar
        resource:
          requests:
            memory: 128Mi
            cpu: 50m
          limits:
            memory: 256Mi
      - name : csi-rbdplugin
        resource:
          requests:
            memory: 512Mi
            cpu: 250m
          limits:
            memory: 512Mi
      - name : liveness-prometheus
        resource:
          requests:
            memory: 128Mi
            cpu: 50m
          limits:
            memory: 256Mi
  version: v1.16.6
# Ceph cluster helm chart
- name: rook-ceph-cluster
  namespace: rook-ceph
  releaseName: rook-ceph-cluster
  repo: https://charts.rook.io/release
  version: v1.16.6
  valuesInline:
    # Size is 3 because of the triple node cluster - which consists of the 3 amd64 machines.
    configOverride: |
      [global]
      osd_pool_default_size = 3

    # Dashboard not needed; grafana dashboards are enough.
    dashboard:
      enabled: false
    ingress:
      enabled: false

    # HA mon/mgr replica count (yes, 2 for mgr)
    mon:
      allowMultiplePerNode: false
      count: 3
    mgr:
      allowMultiplePerNode: false
      count: 2

    # TODO: Reduce overall memory limits that are affecting other deployments in the cluster.
    #       Preventing them from deploying (bad).
    
    #
    # Monitor deployments may contain an anti-affinity rule for avoiding monitor
    # collocation on the same node. This is a required rule when host network is used
    # or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
    # preferred rule with weight: 50.
    #
    # The above placement information can also be specified for mon, mgr, cleanup and osd components
    #
    # mon:
    # mgr:
    # cleanup:
    # osd:
    # TODO: Look at removing CephPGImbalance alarm; it is not needed for now, 
    #       since not all OSDs have the same capacity on purpose. -> Silence it on alertmanager instead of deleting/muting it
    monitoring:
      createPrometheusRules: true
      enabled: true

    nfs:
      enabled: false

    operatorNamespace: rook-ceph

    toolbox:
      enabled: false
