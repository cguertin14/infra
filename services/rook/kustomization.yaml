apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: rook-ceph

resources:
- resources/namespace.yml

patches:
- path: patches/storageclasses.yml
- path: patches/cluster-affinity.yml
- path: patches/cluster-resources.yml
- path: patches/prometheus-rules.yml
  target:
    group: monitoring.coreos.com
    version: v1
    kind: PrometheusRule
    name: prometheus-ceph-rules
    namespace: rook-ceph

labels:
- includeSelectors: true
  pairs:
    app.kubernetes.io/name: rook-ceph

helmCharts:
# Operator helm chart
- name: rook-ceph
  namespace: rook-ceph
  releaseName: rook-ceph
  repo: https://charts.rook.io/release
  version: v1.19.1
  valuesInline:
    crds:
      enabled: true
    monitoring:
      enabled: true

    # Adjust plugin pods resources for less memory usage
    csi:
      csiCephFSPluginResource: |
        - name : driver-registrar
          resource:
            requests:
              memory: 128Mi
              cpu: 50m
            limits:
              memory: 128Mi
        - name : csi-cephfsplugin
          resource:
            requests:
              memory: 128Mi
              cpu: 50m
            limits:
              memory: 128Mi
        - name : liveness-prometheus
          resource:
            requests:
              memory: 128Mi
              cpu: 50m
            limits:
              memory: 128Mi
      csiRBDPluginResource: |
        - name : driver-registrar
          resource:
            requests:
              memory: 128Mi
              cpu: 50m
            limits:
              memory: 128Mi
        - name : csi-rbdplugin
          resource:
            requests:
              memory: 128Mi
              cpu: 50m
            limits:
              memory: 128Mi
        - name : liveness-prometheus
          resource:
            requests:
              memory: 128Mi
              cpu: 50m
            limits:
              memory: 128Mi
# Ceph cluster helm chart
- name: rook-ceph-cluster
  namespace: rook-ceph
  releaseName: rook-ceph-cluster
  repo: https://charts.rook.io/release
  version: v1.19.1
  valuesInline:
    # Size is 3 because of the triple node cluster - which consists of the 3 amd64 machines.
    # Note for bluestore changes: https://github.com/rook/rook/discussions/15403
    configOverride: |
      [global]
      osd_pool_default_size = 3
      bluestore_slow_ops_warn_lifetime = 0

    # Dashboard not needed; grafana dashboards are enough.
    dashboard:
      enabled: false
    ingress:
      enabled: false

    # HA mon/mgr replica count (yes, 2 for mgr)
    mon:
      allowMultiplePerNode: false
      count: 3
    mgr:
      allowMultiplePerNode: false
      count: 2
    #
    # Monitor deployments may contain an anti-affinity rule for avoiding monitor
    # collocation on the same node. This is a required rule when host network is used
    # or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
    # preferred rule with weight: 50.
    #
    # The above placement information can also be specified for mon, mgr, cleanup and osd components
    #
    # mon:
    # mgr:
    # cleanup:
    # osd:
    # TODO: Look at removing CephPGImbalance alarm; it is not needed for now, 
    #       since not all OSDs have the same capacity on purpose. -> Silence it on alertmanager instead of deleting/muting it
    monitoring:
      createPrometheusRules: true
      enabled: true

    nfs:
      enabled: false

    operatorNamespace: rook-ceph

    toolbox:
      enabled: true

    # Deploy a volume snapshot class for block storage
    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
      annotations:
        snapshot.storage.kubernetes.io/is-default-class: "true"
      labels:
        velero.io/csi-volumesnapshot-class: "true"
